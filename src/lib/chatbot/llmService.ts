/**
 * ü§ñ LLM Service - Int√©gration OpenAI GPT
 * Service pour am√©liorer le chatbot avec une LLM
 */

import { ENV_CONFIG } from '@/config/env';

export interface LLMResponse {
  response: string;
  confidence: number;
  reasoning?: string;
}

export class LLMService {
  private apiKey: string;
  private enabled: boolean;
  private baseUrl = 'https://api.openai.com/v1';

  constructor() {
    this.apiKey = ENV_CONFIG.OPENAI_API_KEY;
    this.enabled = ENV_CONFIG.OPENAI_ENABLED && !!this.apiKey;
    
    if (this.enabled) {
      console.log('‚úÖ OpenAI LLM enabled');
      console.log('üîë API Key configured:', this.apiKey.substring(0, 15) + '...');
    } else {
      console.log('‚ùå OpenAI LLM disabled');
      console.log('üîç Debug - OPENAI_ENABLED:', ENV_CONFIG.OPENAI_ENABLED, '| Has Key:', !!this.apiKey);
    }
  }

  /**
   * G√©n√©re une r√©ponse intelligente avec OpenAI GPT
   */
  async generateResponse(
    userMessage: string,
    context: {
      intent?: string;
      entities?: any[];
      conversationHistory?: Array<{ role: 'user' | 'assistant'; content: string }>;
      platformContext?: any;
    }
  ): Promise<LLMResponse | null> {
    if (!this.enabled) {
      console.log('‚ö†Ô∏è LLM Service disabled, skipping');
      return null;
    }

    try {
      console.log('üöÄ Calling OpenAI API with message:', userMessage.substring(0, 50) + '...');
      
      // Construire le syst√®me prompt avec contexte
      const systemPrompt = this.buildSystemPrompt(context);

      // Construire les messages
      const messages = this.buildMessages(systemPrompt, userMessage, context.conversationHistory);

      // Appel API OpenAI
      const response = await fetch(`${this.baseUrl}/chat/completions`, {
        method: 'POST',
        headers: {
          'Content-Type': 'application/json',
          'Authorization': `Bearer ${this.apiKey}`
        },
        body: JSON.stringify({
          model: 'gpt-3.5-turbo',
          messages,
          temperature: 0.7,
          max_tokens: 500,
          stream: false
        })
      });

      if (!response.ok) {
        const errorData = await response.json().catch(() => ({}));
        
        // Special handling for rate limits
        if (response.status === 429) {
          console.warn('‚ö†Ô∏è OpenAI rate limit reached, falling back to NLP');
          return null;
        }
        
        throw new Error(`OpenAI API error: ${response.status} ${response.statusText}`);
      }

      const data = await response.json();
      const aiResponse = data.choices[0]?.message?.content || '';

      console.log('‚úÖ OpenAI response received:', aiResponse.substring(0, 50) + '...');
      return {
        response: aiResponse,
        confidence: 0.8, // LLM responses generally have high confidence
        reasoning: 'Generated by OpenAI GPT'
      };
    } catch (error) {
      console.error('‚ùå LLM Service error:', error);
      return null; // Fallback to NLP engine
    }
  }

  /**
   * Construire le prompt syst√®me avec contexte StudyMarket
   */
  private buildSystemPrompt(context: any): string {
    return `Tu es l'assistant IA de StudyMarket, une plateforme de marketplace √©tudiante.

CONTEXTE PLATEFORME:
- Les √©tudiants peuvent acheter, vendre et √©changer des biens et services
- Cat√©gories disponibles: livres, √©lectronique, v√™tements, mobilier, sports, logement, services
- Fonctionnalit√©s: annonces, messages, favoris, profil, commandes

TON R√îLE:
- R√©pondre de mani√®re amicale et concise
- Guider les utilisateurs vers les bonnes fonctionnalit√©s
- Proposer des suggestions pertinentes
- Proposer de contacter le support si tu ne peux pas r√©pondre

IMPORTANT:
- Reste concis (max 3 lignes par r√©ponse)
- Utilise des emojis avec parcimonie
- Si tu ne sais pas, propose de contacter le support
- Ne donne JAMAIS d'informations sur la s√©curit√© financi√®re ou des conseils l√©gaux

${context.intent ? `Intent d√©tect√©: ${context.intent}` : ''}
${context.entities && context.entities.length > 0 ? `Entit√©s: ${context.entities.map(e => e.type).join(', ')}` : ''}`;
  }

  /**
   * Construire les messages pour l'API
   */
  private buildMessages(
    systemPrompt: string,
    userMessage: string,
    history?: Array<{ role: 'user' | 'assistant'; content: string }>
  ): any[] {
    const messages: any[] = [
      { role: 'system', content: systemPrompt }
    ];

    // Ajouter l'historique r√©cent (max 5 derniers messages)
    if (history && history.length > 0) {
      const recentHistory = history.slice(-5);
      messages.push(...recentHistory);
    }

    // Ajouter le message actuel
    messages.push({ role: 'user', content: userMessage });

    return messages;
  }

  /**
   * D√©cider si l'on doit utiliser la LLM
   */
  shouldUseLLM(nlpConfidence: number, intent: string): boolean {
    // Utiliser LLM si:
    // - Confiance NLP faible (< 0.5)
    // - Intent UNKNOWN
    // - Demandes complexes n√©cessitant du raisonnement
    return this.enabled && (nlpConfidence < 0.5 || intent === 'unknown');
  }
}

// ==================== SINGLETON EXPORT ====================

export const llmService = new LLMService();
